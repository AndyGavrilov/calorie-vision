"""–£—Ç–∏–ª–∏—Ç—ã –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∫–∞–ª–æ—Ä–∏–π–Ω–æ—Å—Ç–∏ –±–ª—é–¥"""

import os
import random
import time
from functools import partial
from typing import Dict, Tuple, Optional

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.optim import AdamW, lr_scheduler
from torch.utils.data import DataLoader
from transformers import AutoModel, AutoTokenizer
import timm
import torchmetrics
import yaml
from tqdm import tqdm

from scripts.dataset import create_data_loaders


def seed_everything(seed: int = 42) -> None:
    """
    –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç seed –¥–ª—è –≤—Å–µ—Ö –±–∏–±–ª–∏–æ—Ç–µ–∫ –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏.
    
    Args:
        seed: –ó–Ω–∞—á–µ–Ω–∏–µ seed
    """
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


class CaloriePredictor(nn.Module):
    """–ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∫–∞–ª–æ—Ä–∏–π–Ω–æ—Å—Ç–∏ –±–ª—é–¥.
    
    –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –≤–∫–ª—é—á–∞–µ—Ç:
    - Vision encoder (EfficientNet-B3) –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
    - Text encoder (DistilBERT) –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å–ø–∏—Å–∫–∞ –∏–Ω–≥—Ä–µ–¥–∏–µ–Ω—Ç–æ–≤
    - Fusion network –¥–ª—è –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–µ–π
    - Regression head –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∫–∞–ª–æ—Ä–∏–π
    """
    
    def __init__(
        self,
        vision_model_name: str = "efficientnet_b3",
        text_model_name: str = "distilbert-base-uncased",
        hidden_dim: int = 256,
        dropout_rate: float = 0.3
    ):
        """
        Args:
            vision_model_name: –ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
            text_model_name: –ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞
            hidden_dim: –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —Å–∫—Ä—ã—Ç—ã—Ö —Å–ª–æ–µ–≤
            dropout_rate: –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç dropout
        """
        super().__init__()
        
        # Vision encoder
        self.vision_encoder = timm.create_model(
            vision_model_name,
            pretrained=True,
            num_classes=0,  # –ë–µ–∑ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–æ–Ω–Ω–æ–π –≥–æ–ª–æ–≤—ã
            global_pool='avg'
        )
        
        # Text encoder
        self.text_encoder = AutoModel.from_pretrained(text_model_name)
        
        # Feature dimensions
        vision_dim = self.vision_encoder.num_features
        text_dim = self.text_encoder.config.hidden_size
        
        # Projection layers
        self.vision_proj = nn.Sequential(
            nn.Linear(vision_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout_rate)
        )
        
        self.text_proj = nn.Sequential(
            nn.Linear(text_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout_rate)
        )
        
        # Fusion and regression layers
        self.fusion = nn.Sequential(
            nn.Linear(hidden_dim * 2, hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout_rate),
            nn.LayerNorm(hidden_dim)
        )
        
        # MLP –¥–ª—è –º–∞—Å—Å–æ–≤–æ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–∞
        self.mass_mlp = nn.Sequential(
            nn.Linear(1, 32),
            nn.ReLU(),
            nn.Dropout(dropout_rate * 0.3)
        )

        # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π —Ä–µ–≥—Ä–µ—Å—Å–æ—Ä —Å —É—á–µ—Ç–æ–º –º–∞—Å—Å—ã
        fusion_dim = hidden_dim + hidden_dim + 32  # vision + text + mass
        self.regressor = nn.Sequential(
            nn.Linear(fusion_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout_rate * 0.5),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(dropout_rate * 0.3),
            nn.Linear(hidden_dim // 2, 1)
        )
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤
        self._initialize_weights()
    
    def _initialize_weights(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤ –Ω–æ–≤–æ–≥–æ —Å–ª–æ–µ–≤."""
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.LayerNorm):
                nn.init.constant_(m.bias, 0)
                nn.init.constant_(m.weight, 1.0)
    
    def forward(
        self,
        images: torch.Tensor,
        text_input_ids: torch.Tensor,
        text_attention_mask: torch.Tensor,
        mass_features: torch.Tensor
    ) -> torch.Tensor:
        """
        Forward pass –º–æ–¥–µ–ª–∏ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –º–∞—Å—Å–æ–≤–æ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–∞.
        
        Args:
            images: –¢–µ–Ω–∑–æ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π [batch_size, channels, height, width]
            text_input_ids: –¢–æ–∫–µ–Ω—ã —Ç–µ–∫—Å—Ç–∞ [batch_size, max_length]
            text_attention_mask: –ú–∞—Å–∫–∞ –≤–Ω–∏–º–∞–Ω–∏—è [batch_size, max_length]
            mass_features: –õ–æ–≥-–Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –º–∞—Å—Å—ã [batch_size]
            
        Returns:
            –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∫–∞–ª–æ—Ä–∏–π –Ω–∞ 100–≥ [batch_size]
        """
        # Vision processing
        vision_features = self.vision_encoder(images)  # [batch_size, vision_dim]
        vision_proj = self.vision_proj(vision_features)  # [batch_size, hidden_dim]
        
        # Text processing
        text_output = self.text_encoder(
            input_ids=text_input_ids,
            attention_mask=text_attention_mask
        )
        text_features = text_output.last_hidden_state[:, 0, :]  # [batch_size, text_dim]
        text_proj = self.text_proj(text_features)  # [batch_size, hidden_dim]
        
        # Mass processing
        # [batch_size, 1] -> [batch_size, 32]
        mass_emb = self.mass_mlp(mass_features.unsqueeze(1))

        # Fusion –≤—Å–µ—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
        # [batch_size, hidden_dim * 2 + 32]
        fused = torch.cat([vision_proj, text_proj, mass_emb], dim=1)
        
        # Regression
        calories_pred_per_100g = self.regressor(fused)  # [batch_size, 1]
        
        return calories_pred_per_100g.squeeze(-1)  # [batch_size]


def create_optimizer_and_scheduler(
    model: nn.Module,
    config: Dict
) -> Tuple[torch.optim.Optimizer, torch.optim.lr_scheduler._LRScheduler]:
    """
    –°–æ–∑–¥–∞–µ—Ç –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –∏ –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫ learning rate.
    
    Args:
        model: –ú–æ–¥–µ–ª—å –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
        config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –æ–±—É—á–µ–Ω–∏—è
        
    Returns:
        Tuple —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–æ–º –∏ –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫–æ–º learning rate
    """
    # –†–∞–∑–Ω—ã–µ learning rates –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
    vision_params = [p for n, p in model.named_parameters() if 'vision_encoder' in n]
    text_params = [p for n, p in model.named_parameters() if 'text_encoder' in n]
    other_params = [p for n, p in model.named_parameters() 
                   if 'vision_encoder' not in n and 'text_encoder' not in n]
    
    param_groups = [
        {'params': vision_params, 'lr': config['vision_lr'], 'name': 'vision'},
        {'params': text_params, 'lr': config['text_lr'], 'name': 'text'},
        {'params': other_params, 'lr': config['fusion_lr'], 'name': 'fusion'}
    ]
    
    optimizer = AdamW(param_groups, weight_decay=config['weight_decay'])
    
    # –ù–æ–≤—ã–π –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫: warmup + cosine annealing –±–µ–∑ —Ä–µ—Å—Ç–∞—Ä—Ç–æ–≤
    import math
    warmup_epochs = config.get('warmup_epochs', 5)
    min_lr = config.get('min_lr', 1e-6)
    total_epochs = config['epochs']

    def lr_lambda(epoch):
        if epoch < warmup_epochs:
            return (epoch + 1) / warmup_epochs
        # –õ–∏–Ω–µ–π–Ω–æ–µ —Å–æ–æ—Ç–Ω–µ—Å–µ–Ω–∏–µ –∫ –∫–æ—Å–∏–Ω—É—Å—É –±–µ–∑ —Ä–µ—Å—Ç–∞—Ä—Ç–æ–≤
        t = (epoch - warmup_epochs) / max(1, total_epochs - warmup_epochs)
        return 0.5 * (1 + math.cos(math.pi * t))  # –æ—Ç 1 –¥–æ 0

    scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda)
    
    return optimizer, scheduler


def train_epoch(
    model: nn.Module,
    train_loader: DataLoader,
    optimizer: torch.optim.Optimizer,
    criterion: nn.Module,
    device: torch.device,
    config: Dict
) -> Tuple[float, float]:
    """
    –û–±—É—á–∞–µ—Ç –º–æ–¥–µ–ª—å –Ω–∞ –æ–¥–Ω–æ–π —ç–ø–æ—Ö–µ.
    
    Args:
        model: –ú–æ–¥–µ–ª—å –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
        train_loader: –ó–∞–≥—Ä—É–∑—á–∏–∫ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
        optimizer: –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä
        criterion: –§—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å
        device: –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π
        config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
        
    Returns:
        Tuple —Å —Å—Ä–µ–¥–Ω–µ–π loss –∏ MAE –¥–ª—è —ç–ø–æ—Ö–∏
    """
    model.train()
    total_loss = 0.0
    total_mae = 0.0
    num_batches = len(train_loader)
    
    progress_bar = tqdm(train_loader, desc="Training", leave=False)
    
    accumulate_steps = int(config.get('accumulate_steps', 1))
    optimizer.zero_grad()
    for batch_idx, batch in enumerate(progress_bar):
        optimizer.zero_grad()
        
        # –ü–µ—Ä–µ–º–µ—â–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
        images = batch['images'].to(device)
        text_input_ids = batch['text_input_ids'].to(device)
        text_attention_mask = batch['text_attention_masks'].to(device)
        calories_per_100g_true = batch['calories_per_100g'].to(device)
        mass_features = batch['mass_features'].to(device)
        
        # Forward pass
        calories_pred_per_100g = model(
            images, text_input_ids, text_attention_mask, mass_features)
        
        # –ù–æ–≤—ã–π –ª–æ—Å—Å: MAE + 0.2*MSE –¥–ª—è –ª—É—á—à–µ–π —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏
        import torch.nn.functional as F
        mae_loss = F.l1_loss(calories_pred_per_100g, calories_per_100g_true)
        mse_loss = F.mse_loss(calories_pred_per_100g, calories_per_100g_true)
        loss = mae_loss + 0.2 * mse_loss
        
        # Backward pass
        (loss / accumulate_steps).backward()
        
        # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∏ –∫–ª–∏–ø–ø–∏–Ω–≥ –ø–æ —à–∞–≥–∞–º –∞–∫–∫—É–º—É–ª—è—Ü–∏–∏
        if (batch_idx + 1) % accumulate_steps == 0:
            torch.nn.utils.clip_grad_norm_(model.parameters(), config['grad_clip'])
            optimizer.step()
            optimizer.zero_grad()
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ - –∫–æ–Ω–≤–µ—Ä—Å–∏—è –≤ –æ–±—â–∏–µ –∫–∞–ª–æ—Ä–∏–∏ –¥–ª—è –ø–æ–Ω—è—Ç–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫
        with torch.no_grad():
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –∫–∞–ª–æ—Ä–∏–∏ –Ω–∞ 100–≥ –æ–±—Ä–∞—Ç–Ω–æ –≤ –æ–±—â–∏–µ –∫–∞–ª–æ—Ä–∏–∏ –¥–ª—è –º–µ—Ç—Ä–∏–∫–∏
            masses = batch['masses'].to(device)
            calories_pred_total = calories_pred_per_100g * (masses / 100.0)
            calories_true_total = calories_per_100g_true * (masses / 100.0)
            mae = F.l1_loss(calories_pred_total, calories_true_total).item()
        
        total_loss += loss.item()
        total_mae += mae
        
        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞–∂–¥—ã–µ N —à–∞–≥–æ–≤
        log_every_n_steps = config.get('logging', {}).get('log_every_n_steps', 10)
        progress_info = {
            'Loss': f'{loss.item():.4f}',
            'MAE': f'{mae:.2f} –∫–∫–∞–ª'
        }
        if batch_idx % log_every_n_steps == 0:
            progress_info['Step'] = batch_idx
            
        progress_bar.set_postfix(progress_info)
    
    return total_loss / num_batches, total_mae / num_batches


def validate_epoch(
    model: nn.Module,
    val_loader: DataLoader,
    criterion: nn.Module,
    device: torch.device
) -> Tuple[float, float, float]:
    """
    –í–∞–ª–∏–¥–∏—Ä—É–µ—Ç –º–æ–¥–µ–ª—å –Ω–∞ –æ–¥–Ω–æ–π —ç–ø–æ—Ö–µ.
    
    Args:
        model: –ú–æ–¥–µ–ª—å –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏
        val_loader: –ó–∞–≥—Ä—É–∑—á–∏–∫ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏
        criterion: –§—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å
        device: –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π
        
    Returns:
        Tuple —Å loss, MAE –∏ MSE –¥–ª—è —ç–ø–æ—Ö–∏
    """
    model.eval()
    total_loss = 0.0
    total_mae = 0.0
    total_mse = 0.0
    num_batches = len(val_loader)
    
    with torch.no_grad():
        for batch in tqdm(val_loader, desc="Validation", leave=False):
            # –ü–µ—Ä–µ–º–µ—â–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
            images = batch['images'].to(device)
            text_input_ids = batch['text_input_ids'].to(device)
            text_attention_mask = batch['text_attention_masks'].to(device)
            calories_per_100g_true = batch['calories_per_100g'].to(device)
            masses = batch['masses'].to(device)
            mass_features = batch['mass_features'].to(device)
            
            # Forward pass
            calories_pred_per_100g = model(
                images, text_input_ids, text_attention_mask, mass_features)

            # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –æ–±—Ä–∞—Ç–Ω–æ –≤ –æ–±—â–∏–µ –∫–∞–ª–æ—Ä–∏–∏ –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏
            calories_pred_total = calories_pred_per_100g * (masses / 100.0)
            calories_true_total = calories_per_100g_true * (masses / 100.0)
            
            # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫
            mae_loss = F.l1_loss(calories_pred_per_100g,
                                 calories_per_100g_true)
            mse_loss = F.mse_loss(calories_pred_per_100g,
                                  calories_per_100g_true)
            loss = mae_loss + 0.2 * mse_loss

            # –ú–µ—Ç—Ä–∏–∫–∏ –≤ –æ–±—â–∏—Ö –∫–∞–ª–æ—Ä–∏—è—Ö –¥–ª—è –ø–æ–Ω—è—Ç–Ω–æ—Å—Ç–∏
            mae = F.l1_loss(calories_pred_total, calories_true_total).item()
            mse = F.mse_loss(calories_pred_total, calories_true_total).item()
            
            total_loss += loss.item()
            total_mae += mae
            total_mse += mse
    
    return total_loss / num_batches, total_mae / num_batches, total_mse / num_batches




def train_model(
    model: nn.Module,
    optimizer: torch.optim.Optimizer,
    scheduler: torch.optim.lr_scheduler._LRScheduler,
    criterion: nn.Module,
    train_loader: DataLoader,
    val_loader: DataLoader,
    config: Dict,
    device: torch.device,
    checkpoint_callback: Optional[callable] = None
) -> Dict[str, float]:
    """
    –í—ã–ø–æ–ª–Ω—è–µ—Ç –ø–æ–ª–Ω—ã–π —Ü–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ —Å –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ–º –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º.
    
    Args:
        model: –ú–æ–¥–µ–ª—å –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
        optimizer: –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä
        scheduler: –ü–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫ learning rate
        criterion: –§—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å
        train_loader: –ó–∞–≥—Ä—É–∑—á–∏–∫ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        val_loader: –ó–∞–≥—Ä—É–∑—á–∏–∫ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è
        device: –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π
        checkpoint_callback: –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –æ–±—Ä–∞—Ç–Ω–æ–≥–æ –≤—ã–∑–æ–≤–∞
        
    Returns:
        –°–ª–æ–≤–∞—Ä—å —Å –ª—É—á—à–∏–º–∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏
    """
    print("=" * 70)
    print("–ù–ê–ß–ê–õ–û –û–ë–£–ß–ï–ù–ò–Ø –ú–û–î–ï–õ–ò")
    print("=" * 70)
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –ª—É—á—à–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    best_mae = float('inf')
    best_epoch = 0
    train_losses = []
    val_losses = []
    train_maes = []
    val_maes = []
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã —Ä–∞–∑–º–æ—Ä–æ–∑–∫–∏
    freeze_vision_epochs = int(config.get('freeze_vision_epochs', 0))
    freeze_text_epochs = int(config.get('freeze_text_epochs', 0))

    # –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ
    start_time = time.time()
    # –ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ —Ä–∞–∑–º–æ—Ä–æ–∑–∫–∏, –æ–ø—Ä–µ–¥–µ–ª–µ–Ω—ã –≤—ã—à–µ
    
    for epoch in range(config['epochs']):
        epoch_start = time.time()
        
        print(f"\n–≠–ø–æ—Ö–∞ {epoch + 1}/{config['epochs']}")
        print("-" * 50)
        
        # –†–∞–∑–º–æ—Ä–æ–∑–∫–∞ —ç–Ω–∫–æ–¥–µ—Ä–æ–≤ –≤ —É–∫–∞–∑–∞–Ω–Ω—ã–µ —ç–ø–æ—Ö–∏
        if epoch == freeze_vision_epochs and freeze_vision_epochs > 0:
            for p in model.vision_encoder.parameters():
                p.requires_grad = True
            print("–†–∞–∑–º–æ—Ä–æ–∑–∏–ª–∏ vision encoder")
        if epoch == freeze_text_epochs and freeze_text_epochs > 0:
            for p in model.text_encoder.parameters():
                p.requires_grad = True
            print("–†–∞–∑–º–æ—Ä–æ–∑–∏–ª–∏ text encoder")
        
        # –û–ë–£–ß–ï–ù–ò–ï
        train_loss, train_mae = train_epoch(
            model, train_loader, optimizer, criterion, device, config
        )
        
        # –í–ê–õ–ò–î–ê–¶–ò–Ø
        val_loss, val_mae, val_mse = validate_epoch(
            model, val_loader, criterion, device
        )
        
        # –û–±–Ω–æ–≤–ª—è–µ–º learning rate
        scheduler.step()
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
        train_losses.append(train_loss)
        val_losses.append(val_loss)
        train_maes.append(train_mae)
        val_maes.append(val_mae)
        
        # –í—ã—á–∏—Å–ª—è–µ–º –≤—Ä–µ–º—è —ç–ø–æ—Ö–∏
        epoch_time = time.time() - epoch_start
        
        print(f"Train - Loss: {train_loss:.4f}, MAE: {train_mae:.2f} –∫–∫–∞–ª")
        print(f"Val   - Loss: {val_loss:.4f}, MAE: {val_mae:.2f} –∫–∫–∞–ª, MSE: {val_mse:.2f}")
        print(f"LR: {optimizer.param_groups[0]['lr']:.2e}")
        print(f"–í—Ä–µ–º—è —ç–ø–æ—Ö–∏: {epoch_time:.1f} —Å–µ–∫")
        
        # –í—ã–∑—ã–≤–∞–µ–º callback —Ñ—É–Ω–∫—Ü–∏—é –µ—Å–ª–∏ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–∞
        checkpoint_data = {
            'epoch': epoch,
            'train_loss': train_loss,
            'train_mae': train_mae,
            'val_loss': val_loss,
            'val_mae': val_mae,
            'val_mse': val_mse,
            'best_mae': best_mae,
            'best_epoch': best_epoch,
            'is_best': False,
            'config': config
        }
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å—Ç–∞–ª–∞ –ª–∏ —ç—Ç–æ –ª—É—á—à–µ–π –º–æ–¥–µ–ª—å—é
        if val_mae < best_mae:
            best_mae = val_mae
            best_epoch = epoch
            checkpoint_data['is_best'] = True
            
            print(f"‚úÖ –ù–û–í–ê–Ø –õ–£–ß–®–ê–Ø –ú–û–î–ï–õ–¨! MAE: {val_mae:.2f} –∫–∫–∞–ª")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç–∏–∂–µ–Ω–∏–µ —Ü–µ–ª–∏
            if val_mae < config['target_mae']:
                print(f"üéØ –¶–ï–õ–¨ –î–û–°–¢–ò–ì–ù–£–¢–ê! MAE < {config['target_mae']} –∫–∫–∞–ª")
        
        # –í—ã–∑—ã–≤–∞–µ–º callback –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–∏
        if checkpoint_callback:
            checkpoint_callback(model, optimizer, scheduler, checkpoint_data, train_losses, val_losses, train_maes, val_maes)
        
        # Early stopping
        if epoch - best_epoch > config['early_stopping_patience']:
            print(f"\nüõë Early stopping –Ω–∞ —ç–ø–æ—Ö–µ {epoch + 1}")
            break
    
    total_time = time.time() - start_time
    print(f"\n{'='*70}")
    print("–û–ë–£–ß–ï–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û!")
    print(f"{'='*70}")
    print(f"–õ—É—á—à–∞—è MAE: {best_mae:.2f} –∫–∫–∞–ª")
    print(f"–õ—É—á—à–∞—è —ç–ø–æ—Ö–∞: {best_epoch + 1}")
    print(f"–û–±—â–µ–µ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è: {total_time/3600:.2f} —á–∞—Å–æ–≤")
    
    return {
        'best_mae': best_mae,
        'best_epoch': best_epoch,
        'train_losses': train_losses,
        'val_losses': val_losses,
        'train_maes': train_maes,
        'val_maes': val_maes,
        'total_time': total_time
    }


def create_checkpoint_callback(config: Dict, device: torch.device):
    """
    –°–æ–∑–¥–∞–µ—Ç —Ñ—É–Ω–∫—Ü–∏—é –æ–±—Ä–∞—Ç–Ω–æ–≥–æ –≤—ã–∑–æ–≤–∞ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —á–µ–∫–ø–æ–∏–Ω—Ç–æ–≤.
    
    Args:
        config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –º–æ–¥–µ–ª–∏
        device: –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π
        
    Returns:
        –§—É–Ω–∫—Ü–∏—è callback –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π
    """
    os.makedirs(config['output_dir'], exist_ok=True)
    
    def checkpoint_callback(model, optimizer, scheduler, data, train_losses, val_losses, train_maes, val_maes):
        epoch = data['epoch']
        epoch_num = epoch + 1
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞
        save_checkpoints = config['logging']['save_checkpoints']
        checkpoint_interval = config['logging']['checkpoint_interval']
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —á–µ–∫–ø–æ–∏–Ω—Ç—ã —ç–ø–æ—Ö —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–æ –∏ –ø–æ —Ä–∞—Å–ø–∏—Å–∞–Ω–∏—é
        if save_checkpoints and checkpoint_interval >= 1:
            if epoch_num % checkpoint_interval == 0:
                epoch_model_path = os.path.join(config['output_dir'], f'model_epoch_{epoch_num:03d}.pth')
                torch.save({
                    'model_state_dict': model.state_dict(),
                    'optimizer_state_dict': optimizer.state_dict(),
                    'scheduler_state_dict': scheduler.state_dict(),
                    'epoch': epoch,
                    'val_mae': data['val_mae'],
                    'val_loss': data['val_loss'],
                    'train_mae': data['train_mae'],
                    'train_loss': data['train_loss'],
                    'config': config
                }, epoch_model_path)
                print(f"–ú–æ–¥–µ–ª—å —ç–ø–æ—Ö–∏ {epoch_num} —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {epoch_model_path}")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å –æ—Ç–¥–µ–ª—å–Ω–æ (–≤—Å–µ–≥–¥–∞ –ø—Ä–∏ —É–ª—É—á—à–µ–Ω–∏–∏ MAE)
        if data['is_best']:
            best_model_path = os.path.join(config['output_dir'], 'best_model.pth')
            torch.save({
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'scheduler_state_dict': scheduler.state_dict(),
                'epoch': epoch,
                'best_mae': data['val_mae'],
                'val_loss': data['val_loss'],
                'train_mae': data['train_mae'],
                'train_loss': data['train_loss'],
                'config': config,
                'train_losses': train_losses,
                'val_losses': val_losses,
                'train_maes': train_maes,
                'val_maes': val_maes,
                'is_best_model': True
            }, best_model_path)
            print(f"–õ–£–ß–®–ê–Ø –º–æ–¥–µ–ª—å –æ–±–Ω–æ–≤–ª–µ–Ω–∞: {best_model_path} (MAE: {data['val_mae']:.2f})")
    
    return checkpoint_callback


def train(
    config_path: str,
    output_dir: str = "models"
) -> Dict[str, float]:
    """
    –ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏.
    
    Args:
        config_path: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        output_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–∏
        
    Returns:
        –°–ª–æ–≤–∞—Ä—å —Å –ª—É—á—à–∏–º–∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏
    """
    # –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    
    # –£—Å—Ç–∞–Ω–æ–≤–∫–∞ seed –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏
    seed_everything(config['seed'])
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –¥–ª—è –º–æ–¥–µ–ª–∏
    os.makedirs(output_dir, exist_ok=True)
    
    # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞ –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞
    device_setting = config.get('device', 'auto')
    if device_setting == 'auto':
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    elif device_setting == 'cuda':
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    elif device_setting == 'cpu':
        device = torch.device('cpu')
    else:
        device = torch.device(device_setting)
        
    print(f"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}")
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ –ø—É—Ç–µ–π –≤ –∫–æ–Ω—Ñ–∏–≥–µ
    print("–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
    import pandas as pd
    
    data_paths = config.get('data_paths', {
        'dish_csv': 'data/dish.csv',
        'ingredients_csv': 'data/ingredients.csv',
        'images_dir': 'data/images'
    })
    
    dish_df = pd.read_csv(data_paths['dish_csv'])
    ingredients_df = pd.read_csv(data_paths['ingredients_csv'])
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –∑–∞–≥—Ä—É–∑—á–∏–∫–æ–≤ –¥–∞–Ω–Ω—ã—Ö —Å –ø–µ—Ä–µ–¥–∞—á–µ–π –∫–æ–Ω—Ñ–∏–≥–∞ –¥–ª—è –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–π
    train_loader, val_loader = create_data_loaders(
        dish_df=dish_df,
        ingredients_df=ingredients_df,
        image_dir=data_paths['images_dir'],
        batch_size=config['batch_size'],
        num_workers=config['num_workers'],
        config=config  # –ü–µ—Ä–µ–¥–∞–µ–º –∫–æ–Ω—Ñ–∏–≥ –¥–ª—è –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–π –∏ image_size
    )
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
    print("–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏...")
    model = CaloriePredictor(
        vision_model_name=config['vision_model'],
        text_model_name=config['text_model'],
        hidden_dim=config['hidden_dim'],
        dropout_rate=config['dropout_rate']
    ).to(device)
    
    # –≠—Ç–∞–ø–Ω–∞—è —Ä–∞–∑–º–æ—Ä–æ–∑–∫–∞ —ç–Ω–∫–æ–¥–µ—Ä–æ–≤ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞)
    freeze_vision_epochs = int(config.get('freeze_vision_epochs', 0))
    freeze_text_epochs = int(config.get('freeze_text_epochs', 0))
    if freeze_vision_epochs > 0:
        for p in model.vision_encoder.parameters():
            p.requires_grad = False
    if freeze_text_epochs > 0:
        for p in model.text_encoder.parameters():
            p.requires_grad = False
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã —Ä–∞–∑–º–æ—Ä–æ–∑–∫–∏
    freeze_vision_epochs = int(config.get('freeze_vision_epochs', 0))
    freeze_text_epochs = int(config.get('freeze_text_epochs', 0))
    if freeze_vision_epochs > 0:
        for p in model.vision_encoder.parameters():
            p.requires_grad = False
        print(f"–ó–∞–º–æ—Ä–æ–∑–∏–ª–∏ vision encoder –Ω–∞ {freeze_vision_epochs} —ç–ø–æ—Ö")
    if freeze_text_epochs > 0:
        for p in model.text_encoder.parameters():
            p.requires_grad = False
        print(f"–ó–∞–º–æ—Ä–æ–∑–∏–ª–∏ text encoder –Ω–∞ {freeze_text_epochs} —ç–ø–æ—Ö")

    # –°–æ–∑–¥–∞–Ω–∏–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞ –∏ –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫–∞
    optimizer, scheduler = create_optimizer_and_scheduler(model, config)
    
    # –§—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å
    criterion = nn.SmoothL1Loss(reduction='mean')
    
    # –û–±—É—á–µ–Ω–∏–µ
    print("–ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è...")
    best_mae = float('inf')
    best_metrics = {}
    
    for epoch in range(config['epochs']):
        print(f"\n–≠–ø–æ—Ö–∞ {epoch + 1}/{config['epochs']}")
        
        # –û–±—É—á–µ–Ω–∏–µ
        train_loss, train_mae = train_epoch(
            model, train_loader, optimizer, criterion, device, config
        )
        
        # –í–∞–ª–∏–¥–∞—Ü–∏—è
        val_loss, val_mae, val_mse = validate_epoch(
            model, val_loader, criterion, device
        )
        
        # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ learning rate
        scheduler.step()
        
        print(f"Train - Loss: {train_loss:.4f}, MAE: {train_mae:.2f}")
        print(f"Val   - Loss: {val_loss:.4f}, MAE: {val_mae:.2f}, MSE: {val_mse:.2f}")
        print(f"Learning rate: {optimizer.param_groups[0]['lr']:.2e}")
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏
        if val_mae < best_mae:
            best_mae = val_mae
            best_metrics = {
                'val_loss': val_loss,
                'val_mae': val_mae,
                'val_mse': val_mse,
                'train_loss': train_loss,
                'train_mae': train_mae,
                'epoch': epoch
            }
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
            model_path = os.path.join(output_dir, 'best_model.pth')
            torch.save({
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'scheduler_state_dict': scheduler.state_dict(),
                'epoch': epoch,
                'best_mae': best_mae,
                'config': config
            }, model_path)
            
            print(f"–ù–æ–≤–∞—è –ª—É—á—à–∞—è –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞! MAE: {val_mae:.2f}")
        
        # Early stopping
        if epoch - best_metrics['epoch'] > config['early_stopping_patience']:
            print(f"Early stopping –Ω–∞ —ç–ø–æ—Ö–µ {epoch + 1}")
            break
    
    print(f"\n–û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
    print(f"–õ—É—á—à–∞—è MAE: {best_mae:.2f}")
    print(f"–õ—É—á—à–∞—è —ç–ø–æ—Ö–∞: {best_metrics['epoch'] + 1}")
    
    return best_metrics


if __name__ == '__main__':
    # –ü—Ä–∏–º–µ—Ä –∑–∞–ø—É—Å–∫–∞
    import sys
    from pathlib import Path
    
    # –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–Ω–µ–≤—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –≤ –ø—É—Ç—å
    root_path = Path(__file__).parent.parent
    sys.path.append(str(root_path))
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏ –∑–∞–ø—É—Å–∫–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ
    metrics = train(
        config_path=str(root_path / "configs/config.yaml"),
        output_dir="models"
    )
    print("–§–∏–Ω–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏:", metrics)
